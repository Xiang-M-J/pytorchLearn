{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. build_vocab.py","metadata":{"id":"ZsyvFYe9ucIF"}},{"cell_type":"code","source":"import nltk\nimport pickle\nimport argparse\nfrom collections import Counter\nfrom pycocotools.coco import COCO\n\nnltk.download('punkt')\nclass Vocabulary(object):\n    \"\"\"Simple vocabulary wrapper.\"\"\"\n\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n\n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx['<unk>']\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.word2idx)\n\n\ndef build_vocab(json, threshold):\n    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n    coco = COCO(json)\n    counter = Counter()\n    ids = coco.anns.keys()\n    for i, id in enumerate(ids):\n        caption = str(coco.anns[id]['caption'])\n        tokens = nltk.tokenize.word_tokenize(caption.lower())\n        counter.update(tokens)\n\n        if (i + 1) % 1000 == 0:\n            print(\"[{}/{}] Tokenized the captions.\".format(i + 1, len(ids)))\n\n    # If the word frequency is less than 'threshold', then the word is discarded.\n    words = [word for word, cnt in counter.items() if cnt >= threshold]\n\n    # Create a vocab wrapper and add some special tokens.\n    vocab = Vocabulary()\n    vocab.add_word('<pad>')\n    vocab.add_word('<start>')\n    vocab.add_word('<end>')\n    vocab.add_word('<unk>')\n\n    # Add the words to the vocabulary.\n    for i, word in enumerate(words):\n        vocab.add_word(word)\n    return vocab\n\n\ndef main(args):\n    vocab = build_vocab(json=args['caption_path'], threshold=args['threshold'])\n    vocab_path = args['vocab_path']\n    with open(vocab_path, 'wb') as f:\n        pickle.dump(vocab, f)\n    print(\"Total vocabulary size: {}\".format(len(vocab)))\n    print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))\n\n\nargs = {'caption_path': 'data/annotations/captions_train2014.json', 'threshold':4,\n        'vocab_path': './data/vocab.pkl'}\nprint(args['caption_path'])\n\n# main(args)\n","metadata":{"id":"LZB3EUHyup8N","execution":{"iopub.status.busy":"2021-12-04T02:58:26.776057Z","iopub.execute_input":"2021-12-04T02:58:26.776841Z","iopub.status.idle":"2021-12-04T02:58:26.793429Z","shell.execute_reply.started":"2021-12-04T02:58:26.77679Z","shell.execute_reply":"2021-12-04T02:58:26.792731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install pycocotools","metadata":{"execution":{"iopub.status.busy":"2021-12-04T02:57:24.443712Z","iopub.execute_input":"2021-12-04T02:57:24.44399Z","iopub.status.idle":"2021-12-04T02:57:41.34576Z","shell.execute_reply.started":"2021-12-04T02:57:24.443963Z","shell.execute_reply":"2021-12-04T02:57:41.3449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. data_loader.py","metadata":{"id":"rc6n8t38uNGE"}},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nimport os\nimport pickle\nimport numpy as np\nimport nltk\nfrom PIL import Image\n\nfrom pycocotools.coco import COCO\n\n\nclass CocoDataset(data.Dataset):\n    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n    def __init__(self, root, json, vocab, transform=None):\n        \"\"\"Set the path for images, captions and vocabulary wrapper.\n        \n        Args:\n            root: image directory.\n            json: coco annotation file path.\n            vocab: vocabulary wrapper.\n            transform: image transformer.\n        \"\"\"\n        self.root = root\n        self.coco = COCO(json)\n        self.ids = list(self.coco.anns.keys())\n        self.vocab = vocab\n        self.transform = transform\n\n    def __getitem__(self, index):\n        \"\"\"Returns one data pair (image and caption).\"\"\"\n        coco = self.coco\n        vocab = self.vocab\n        ann_id = self.ids[index]\n        caption = coco.anns[ann_id]['caption']\n        img_id = coco.anns[ann_id]['image_id']\n        path = coco.loadImgs(img_id)[0]['file_name']\n\n        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n        if self.transform is not None:\n            image = self.transform(image)\n\n        # Convert caption (string) to word ids.\n        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n        caption = []\n        caption.append(vocab('<start>'))\n        caption.extend([vocab(token) for token in tokens])\n        caption.append(vocab('<end>'))\n        target = torch.Tensor(caption)\n        return image, target\n\n    def __len__(self):\n        return len(self.ids)\n\n\ndef collate_fn(data):\n    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n    \n    We should build custom collate_fn rather than using default collate_fn, \n    because merging caption (including padding) is not supported in default.\n\n    Args:\n        data: list of tuple (image, caption). \n            - image: torch tensor of shape (3, 256, 256).\n            - caption: torch tensor of shape (?); variable length.\n\n    Returns:\n        images: torch tensor of shape (batch_size, 3, 256, 256).\n        targets: torch tensor of shape (batch_size, padded_length).\n        lengths: list; valid length for each padded caption.\n    \"\"\"\n    # Sort a data list by caption length (descending order).\n    data.sort(key=lambda x: len(x[1]), reverse=True)\n    images, captions = zip(*data)\n\n    # Merge images (from tuple of 3D tensor to 4D tensor).\n    images = torch.stack(images, 0)\n\n    # Merge captions (from tuple of 1D tensor to 2D tensor).\n    lengths = [len(cap) for cap in captions]\n    targets = torch.zeros(len(captions), max(lengths)).long()\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        targets[i, :end] = cap[:end]        \n    return images, targets, lengths\n\ndef get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n    # COCO caption dataset\n    coco = CocoDataset(root=root,\n                       json=json,\n                       vocab=vocab,\n                       transform=transform)\n    \n    # Data loader for COCO dataset\n    # This will return (images, captions, lengths) for each iteration.\n    # images: a tensor of shape (batch_size, 3, 224, 224).\n    # captions: a tensor of shape (batch_size, padded_length).\n    # lengths: a list indicating valid length for each caption. length is (batch_size).\n    data_loader = torch.utils.data.DataLoader(dataset=coco, \n                                              batch_size=batch_size,\n                                              shuffle=shuffle,\n                                              num_workers=num_workers,\n                                              collate_fn=collate_fn)\n    return data_loader","metadata":{"id":"ts47mFOBuHcT","execution":{"iopub.status.busy":"2021-12-04T02:58:30.987916Z","iopub.execute_input":"2021-12-04T02:58:30.988175Z","iopub.status.idle":"2021-12-04T02:58:32.637065Z","shell.execute_reply.started":"2021-12-04T02:58:30.988146Z","shell.execute_reply":"2021-12-04T02:58:32.636331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. model.py","metadata":{"id":"PbCAOdmAub8g"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet152(pretrained=True)\n        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n        self.resnet = nn.Sequential(*modules)\n        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n        \n    def forward(self, images):\n        \"\"\"Extract feature vectors from input images.\"\"\"\n        with torch.no_grad():\n            features = self.resnet(images)\n        features = features.reshape(features.size(0), -1)\n        features = self.bn(self.linear(features))\n        return features\n\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n        super(DecoderRNN, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.max_seg_length = max_seq_length\n        \n    def forward(self, features, captions, lengths):\n        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n        embeddings = self.embed(captions)\n        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n        hiddens, _ = self.lstm(packed)\n        outputs = self.linear(hiddens[0])\n        return outputs\n    \n    def sample(self, features, states=None):\n        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n        sampled_ids = []\n        inputs = features.unsqueeze(1)\n        for i in range(self.max_seg_length):\n            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n            sampled_ids.append(predicted)\n            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n        return sampled_ids","metadata":{"id":"2ZgXfJdRuiEQ","execution":{"iopub.status.busy":"2021-12-04T02:58:57.69863Z","iopub.execute_input":"2021-12-04T02:58:57.698883Z","iopub.status.idle":"2021-12-04T02:58:57.715909Z","shell.execute_reply.started":"2021-12-04T02:58:57.698855Z","shell.execute_reply":"2021-12-04T02:58:57.715147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. resize.py","metadata":{"id":"hOhmpBgWu6ih"}},{"cell_type":"code","source":"import argparse\nimport os\nfrom PIL import Image\n\n\ndef resize_image(image, size):\n    \"\"\"Resize an image to the given size.\"\"\"\n    return image.resize(size, Image.ANTIALIAS)\n\n\ndef resize_images(image_dir, output_dir, size):\n    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    images = os.listdir(image_dir)\n    num_images = len(images)\n    for i, image in enumerate(images):\n        with open(os.path.join(image_dir, image), 'r+b') as f:\n            with Image.open(f) as img:\n                img = resize_image(img, size)\n                img.save(os.path.join(output_dir, image), img.format)\n        if (i + 1) % 100 == 0:\n            print(\"[{}/{}] Resized the images and saved into '{}'.\"\n                  .format(i + 1, num_images, output_dir))\n\n\ndef main(args):\n    image_dir = args['image_dir']\n    output_dir = args['output_dir']\n    image_size = [args['image_size'], args['image_size']]\n    resize_images(image_dir, output_dir, image_size)\n\n\n# if __name__ == '__main__':\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('--image_dir', type=str, default='./data/train2014/',\n#                         help='directory for train images')\n#     parser.add_argument('--output_dir', type=str, default='./data/resized2014/',\n#                         help='directory for saving resized images')\n#     parser.add_argument('--image_size', type=int, default=256,\n#                         help='size for image after processing')\n\nargs = {'image_dir': './data/train2014/', 'output_dir': './data/resized2014/',\n        'image_size': 256}\n# main(args)\n","metadata":{"id":"TDi7Xeh1u9pR","execution":{"iopub.status.busy":"2021-12-04T02:56:27.789979Z","iopub.status.idle":"2021-12-04T02:56:27.79086Z","shell.execute_reply.started":"2021-12-04T02:56:27.790629Z","shell.execute_reply":"2021-12-04T02:56:27.790653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. train.py","metadata":{"id":"gfMs3EBwvBwv"}},{"cell_type":"code","source":"import argparse\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport os\nimport pickle\n\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torchvision import transforms\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ndef main(args):\n    # Create model directory\n    if not os.path.exists(args['model_path']):\n        os.makedirs(args['model_path'])\n\n    # Image preprocessing, normalization for the pretrained resnet\n    transform = transforms.Compose([\n        transforms.RandomCrop(args['crop_size']),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406),\n                             (0.229, 0.224, 0.225))])\n\n    # Load vocabulary wrapper\n    with open(args['vocab_path'], 'rb') as f:\n        vocab = pickle.load(f)\n\n    # Build data loader\n    data_loader = get_loader(args['image_dir'], args['caption_path'], vocab,\n                             transform, args['batch_size'],\n                             shuffle=True, num_workers=args['num_workers'])\n\n    # Build the models\n    encoder = EncoderCNN(args['embed_size']).to(device)\n    decoder = DecoderRNN(args['embed_size'], args['hidden_size'], len(vocab), args['num_layers']).to(device)\n\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n    optimizer = torch.optim.Adam(params, lr=args['learning_rate'])\n\n    # Train the models\n    total_step = len(data_loader)\n    for epoch in range(args['num_epochs']):\n        for i, (images, captions, lengths) in enumerate(data_loader):\n\n            # Set mini-batch dataset\n            images = images.to(device)\n            captions = captions.to(device)\n            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n\n            # Forward, backward and optimize\n            features = encoder(images)\n            outputs = decoder(features, captions, lengths)\n            loss = criterion(outputs, targets)\n            decoder.zero_grad()\n            encoder.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Print log info\n            if i % args['log_step'] == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n                      .format(epoch, args['num_epochs'], i, total_step, loss.item(), np.exp(loss.item())))\n\n                # Save the model checkpoints\n            if (i + 1) % args['save_step'] == 0:\n                torch.save(decoder.state_dict(), os.path.join(\n                    args['model_path'], 'decoder-{}-{}.ckpt'.format(epoch + 1, i + 1)))\n                torch.save(encoder.state_dict(), os.path.join(\n                    args['model_path'], 'encoder-{}-{}.ckpt'.format(epoch + 1, i + 1)))\n\n\n# if __name__ == '__main__':\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('--model_path', type=str, default='models/', help='path for saving trained models')\n#     parser.add_argument('--crop_size', type=int, default=224, help='size for randomly cropping images')\n#     parser.add_argument('--vocab_path', type=str, default='data/vocab.pkl', help='path for vocabulary wrapper')\n#     parser.add_argument('--image_dir', type=str, default='data/resized2014', help='directory for resized images')\n#     parser.add_argument('--caption_path', type=str, default='data/annotations/captions_train2014.json',\n#                         help='path for train annotation json file')\n#     parser.add_argument('--log_step', type=int, default=10, help='step size for prining log info')\n#     parser.add_argument('--save_step', type=int, default=1000, help='step size for saving trained models')\n\n#     # Model parameters\n#     parser.add_argument('--embed_size', type=int, default=256, help='dimension of word embedding vectors')\n#     parser.add_argument('--hidden_size', type=int, default=512, help='dimension of lstm hidden states')\n#     parser.add_argument('--num_layers', type=int, default=1, help='number of layers in lstm')\n\n#     parser.add_argument('--num_epochs', type=int, default=5)\n#     parser.add_argument('--batch_size', type=int, default=128)\n#     parser.add_argument('--num_workers', type=int, default=2)\n#     parser.add_argument('--learning_rate', type=float, default=0.001)\n#     args = parser.parse_args()\n#     print(args)\nargs = {'model_path':'models/', 'crop_size':224, 'vocab_path':'../input/cocodataset/vocab.pkl','image_dir':'../input/cocodataset/resized2014',\n        'caption_path':'../input/cocodataset/captions_train-val2014/annotations/captions_train2014.json', 'log_step':10, 'save_step': 1000,\n        'embed_size':256,'hidden_size':512, 'num_layers':1, 'num_epochs':4, 'batch_size':128,\n        'num_workers': 2, 'learning_rate': 0.001}\nmain(args)\n","metadata":{"id":"XR3XMS-sGbUE","execution":{"iopub.status.busy":"2021-12-04T03:01:18.567817Z","iopub.execute_input":"2021-12-04T03:01:18.568123Z","iopub.status.idle":"2021-12-04T04:58:08.136884Z","shell.execute_reply.started":"2021-12-04T03:01:18.568091Z","shell.execute_reply":"2021-12-04T04:58:08.136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. sample.py","metadata":{"id":"cgJB4_gZux5W"}},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport pickle\nimport os\nfrom torchvision import transforms\n\nfrom PIL import Image\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef load_image(image_path, transform=None):\n    image = Image.open(image_path).convert('RGB')\n    image = image.resize([224, 224], Image.LANCZOS)\n\n    if transform is not None:\n        image = transform(image).unsqueeze(0)\n\n    return image\n\n\ndef main(args):\n    # Image preprocessing\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406),\n                             (0.229, 0.224, 0.225))])\n\n    # Load vocabulary wrapper\n    with open(args['vocab_path'], 'rb') as f:\n        vocab = pickle.load(f)\n\n    # Build models\n    encoder = EncoderCNN(args['embed_size']).eval()  # eval mode (batchnorm uses moving mean/variance)\n    decoder = DecoderRNN(args['embed_size'], args['hidden_size'], len(vocab), args['num_layers'])\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n\n    # Load the trained model parameters\n    encoder.load_state_dict(torch.load(args['encoder_path']))\n    decoder.load_state_dict(torch.load(args['decoder_path']))\n\n    # Prepare an image\n    image = load_image(args['image'], transform)\n    image_tensor = image.to(device)\n\n    # Generate an caption from the image\n    feature = encoder(image_tensor)\n    sampled_ids = decoder.sample(feature)\n    sampled_ids = sampled_ids[0].cpu().numpy()  # (1, max_seq_length) -> (max_seq_length)\n\n    # Convert word_ids to words\n    sampled_caption = []\n    for word_id in sampled_ids:\n        word = vocab.idx2word[word_id]\n        sampled_caption.append(word)\n        if word == '<end>':\n            break\n    sentence = ' '.join(sampled_caption)\n\n    # Print out the image and the generated caption\n    print(sentence)\n    image = Image.open(args['image'])\n    plt.imshow(np.asarray(image))\n\n# parser = argparse.ArgumentParser()\n# parser.add_argument('--image', type=str, required=True, help='input image for generating caption')\n# parser.add_argument('--encoder_path', type=str, default='models/encoder-5-3000.pkl',\n#                     help='path for trained encoder')\n# parser.add_argument('--decoder_path', type=str, default='models/decoder-5-3000.pkl',\n#                     help='path for trained decoder')\n# parser.add_argument('--vocab_path', type=str, default='data/vocab.pkl', help='path for vocabulary wrapper')\n\n# # Model parameters (should be same as paramters in train.py)\n# parser.add_argument('--embed_size', type=int, default=256, help='dimension of word embedding vectors')\n# parser.add_argument('--hidden_size', type=int, default=512, help='dimension of lstm hidden states')\n# parser.add_argument('--num_layers', type=int, default=1, help='number of layers in lstm')\n\nargs = {'image': True, 'encoder_path': 'models/encoder-5-3000.pkl', 'decoder_path':'models/decoder-5-3000.pkl', 'vocab_path': '../input/cocodataset/vocab.pkl',\n        'embed_size': 256, 'hidden_size': 512, 'num_layers':1}\nmain(args)\n","metadata":{"id":"CvL13bnzu2cK","execution":{"iopub.status.busy":"2021-12-04T05:13:56.31184Z","iopub.execute_input":"2021-12-04T05:13:56.312097Z","iopub.status.idle":"2021-12-04T05:13:56.325596Z","shell.execute_reply.started":"2021-12-04T05:13:56.312067Z","shell.execute_reply":"2021-12-04T05:13:56.324895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! sh download.sh","metadata":{"id":"5MWTIsTFxVbA","execution":{"iopub.status.busy":"2021-12-04T02:56:27.79631Z","iopub.status.idle":"2021-12-04T02:56:27.796879Z","shell.execute_reply.started":"2021-12-04T02:56:27.796647Z","shell.execute_reply":"2021-12-04T02:56:27.796674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"id":"xa-R9qdD0gXV","outputId":"b8b6576c-a752-400b-8edf-480eec2b4952","execution":{"iopub.status.busy":"2021-12-04T02:56:27.797966Z","iopub.status.idle":"2021-12-04T02:56:27.798532Z","shell.execute_reply.started":"2021-12-04T02:56:27.798282Z","shell.execute_reply":"2021-12-04T02:56:27.798309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 打包文件夹","metadata":{"id":"l6AesSQsZqFE"}},{"cell_type":"code","source":"import os\nimport zipfile\n \n \ndef zipDir(dirpath, outFullName):\n    \"\"\"\n    压缩指定文件夹\n    :param dirpath: 目标文件夹路径\n    :param outFullName: 压缩文件保存路径+xxxx.zip\n    :return: 无\n    \"\"\"\n    zip = zipfile.ZipFile(outFullName, \"w\", zipfile.ZIP_DEFLATED)\n    for path, dirnames, filenames in os.walk(dirpath):\n        # 去掉目标跟路径，只对目标文件夹下边的文件及文件夹进行压缩\n        fpath = path.replace(dirpath, '')\n \n        for filename in filenames:\n            zip.write(os.path.join(path, filename), os.path.join(fpath, filename))\n    zip.close()\n \n \nif __name__ == \"__main__\":\n  \n    input_path = \"./models\"\n    output_path = \"./models.zip\"\n \n    zipDir(input_path, output_path)","metadata":{"id":"8qBI1YcISFnx","execution":{"iopub.status.busy":"2021-12-04T05:06:10.374435Z","iopub.execute_input":"2021-12-04T05:06:10.375002Z","iopub.status.idle":"2021-12-04T05:07:05.513538Z","shell.execute_reply.started":"2021-12-04T05:06:10.374961Z","shell.execute_reply":"2021-12-04T05:07:05.512724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 解压","metadata":{"id":"PEfdHFk2aYIO"}},{"cell_type":"code","source":"import zipfile\nf = zipfile.ZipFile(\"./main.zip\",'r') # 原压缩文件在服务器的位置\nfor file in f.namelist():\n  f.extract(file,\"./\") #解压到的位置，./表示当前目录(与此.ipynb文件同一个目录)\nf.close()\n","metadata":{"id":"18KtD1yFabwv","execution":{"iopub.status.busy":"2021-12-04T02:56:27.801243Z","iopub.status.idle":"2021-12-04T02:56:27.801805Z","shell.execute_reply.started":"2021-12-04T02:56:27.801573Z","shell.execute_reply":"2021-12-04T02:56:27.801599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"id":"NjknLRD1TriN","outputId":"b4d17412-71ad-499e-e6c8-6145ba26bf1a","execution":{"iopub.status.busy":"2021-12-04T02:56:41.79128Z","iopub.execute_input":"2021-12-04T02:56:41.791564Z","iopub.status.idle":"2021-12-04T02:56:42.446031Z","shell.execute_reply.started":"2021-12-04T02:56:41.791533Z","shell.execute_reply":"2021-12-04T02:56:42.445184Z"},"trusted":true},"execution_count":null,"outputs":[]}]}