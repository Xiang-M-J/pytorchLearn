{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章 逻辑斯谛回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯谛回归(LR)是经典的分类方法\n",
    "\n",
    "1．逻辑斯谛回归模型是由以下条件概率分布表示的分类模型。逻辑斯谛回归模型可以用于二类或多类分类。\n",
    "\n",
    "$$P(Y=k | x)=\\frac{\\exp \\left(w_{k} \\cdot x\\right)}{1+\\sum_{k=1}^{K-1} \\exp \\left(w_{k} \\cdot x\\right)}, \\quad k=1,2, \\cdots, K-1$$\n",
    "\n",
    "$$P(Y=K | x)=\\frac{1}{1+\\sum_{k=1}^{K-1} \\exp \\left(w_{k} \\cdot x\\right)}$$\n",
    "这里，$x$为输入特征，$w$为特征的权值。\n",
    "\n",
    "逻辑斯谛回归模型源自逻辑斯谛分布，其分布函数$F(x)$是$S$形函数。逻辑斯谛回归模型是由输入的线性函数表示的输出的对数几率模型。\n",
    "\n",
    "2．最大熵模型是由以下条件概率分布表示的分类模型。最大熵模型也可以用于二类或多类分类。\n",
    "\n",
    "$$P_{w}(y | x)=\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)$$\n",
    "$$Z_{w}(x)=\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)$$\n",
    "\n",
    "其中，$Z_w(x)$是规范化因子，$f_i$为特征函数，$w_i$为特征的权值。\n",
    "\n",
    "3．最大熵模型可以由最大熵原理推导得出。最大熵原理是概率模型学习或估计的一个准则。最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。\n",
    "\n",
    "最大熵原理应用到分类模型的学习中，有以下约束最优化问题：\n",
    "\n",
    "$$\\min -H(P)=\\sum_{x, y} \\tilde{P}(x) P(y | x) \\log P(y | x)$$\n",
    "\n",
    "$$s.t.  \\quad P\\left(f_{i}\\right)-\\tilde{P}\\left(f_{i}\\right)=0, \\quad i=1,2, \\cdots, n$$\n",
    " \n",
    " $$\\sum_{y} P(y | x)=1$$\n",
    " \n",
    "求解此最优化问题的对偶问题得到最大熵模型。\n",
    "\n",
    "4．逻辑斯谛回归模型与最大熵模型都属于对数线性模型。\n",
    "\n",
    "5．逻辑斯谛回归模型及最大熵模型学习一般采用极大似然估计，或正则化的极大似然估计。逻辑斯谛回归模型及最大熵模型学习可以形式化为无约束最优化问题。求解该最优化问题的算法有改进的迭代尺度法、梯度下降法、拟牛顿法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "回归模型：$f(x) = \\frac{1}{1+e^{-wx}}$\n",
    "\n",
    "其中wx线性函数：$wx =w_0\\cdot x_0 + w_1\\cdot x_1 + w_2\\cdot x_2 +...+w_n\\cdot x_n,(x_0=1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[:100, [0,1,-1]])\n",
    "    # print(data)\n",
    "    return data[:,:2], data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticReressionClassifier:\n",
    "    def __init__(self, max_iter=200, learning_rate=0.01):\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    def data_matrix(self, X):\n",
    "        data_mat = []\n",
    "        for d in X:\n",
    "            data_mat.append([1.0, *d])\n",
    "        return data_mat\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # label = np.mat(y)\n",
    "        data_mat = self.data_matrix(X)  # m*n\n",
    "        self.weights = np.zeros((len(data_mat[0]), 1), dtype=np.float32)\n",
    "\n",
    "        for iter_ in range(self.max_iter):\n",
    "            for i in range(len(X)):\n",
    "                result = self.sigmoid(np.dot(data_mat[i], self.weights))\n",
    "                error = y[i] - result\n",
    "                self.weights += self.learning_rate * error * np.transpose(\n",
    "                    [data_mat[i]])\n",
    "        print('LogisticRegression Model(learning_rate={},max_iter={})'.format(\n",
    "            self.learning_rate, self.max_iter))\n",
    "\n",
    "    # def f(self, x):\n",
    "    #     return -(self.weights[0] + self.weights[1] * x) / self.weights[2]\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        right = 0\n",
    "        X_test = self.data_matrix(X_test)\n",
    "        for x, y in zip(X_test, y_test):\n",
    "            result = np.dot(x, self.weights)\n",
    "            if (result > 0 and y == 1) or (result < 0 and y == 0):\n",
    "                right += 1\n",
    "        return right / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticReressionClassifier()\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_ponits = np.arange(4, 8)\n",
    "y_ = -(lr_clf.weights[1]*x_ponits + lr_clf.weights[0])/lr_clf.weights[2]\n",
    "plt.plot(x_ponits, y_)\n",
    "\n",
    "#lr_clf.show_graph()\n",
    "plt.scatter(X[:50,0],X[:50,1], label='0')\n",
    "plt.scatter(X[50:,0],X[50:,1], label='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### scikit-learn实例\n",
    "\n",
    "#### sklearn.linear_model.LogisticRegression\n",
    "\n",
    "solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：\n",
    "- a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。\n",
    "- b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "- c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "- d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(clf.coef_, clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_ponits = np.arange(4, 8)\n",
    "y_ = -(clf.coef_[0][0]*x_ponits + clf.intercept_)/clf.coef_[0][1]\n",
    "plt.plot(x_ponits, y_)\n",
    "\n",
    "plt.plot(X[:50, 0], X[:50, 1], 'bo', color='blue', label='0')\n",
    "plt.plot(X[50:, 0], X[50:, 1], 'bo', color='orange', label='1')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大熵模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MaxEntropy:\n",
    "    def __init__(self, EPS=0.005):\n",
    "        self._samples = []\n",
    "        self._Y = set()  # 标签集合，相当去去重后的y\n",
    "        self._numXY = {}  # key为(x,y)，value为出现次数\n",
    "        self._N = 0  # 样本数\n",
    "        self._Ep_ = []  # 样本分布的特征期望值\n",
    "        self._xyID = {}  # key记录(x,y),value记录id号\n",
    "        self._n = 0  # 特征键值(x,y)的个数\n",
    "        self._C = 0  # 最大特征数\n",
    "        self._IDxy = {}  # key为(x,y)，value为对应的id号\n",
    "        self._w = []\n",
    "        self._EPS = EPS  # 收敛条件\n",
    "        self._lastw = []  # 上一次w参数值\n",
    "\n",
    "    def loadData(self, dataset):\n",
    "        self._samples = deepcopy(dataset)\n",
    "        for items in self._samples:\n",
    "            y = items[0]\n",
    "            X = items[1:]\n",
    "            self._Y.add(y)  # 集合中y若已存在则会自动忽略\n",
    "            for x in X:\n",
    "                if (x, y) in self._numXY:\n",
    "                    self._numXY[(x, y)] += 1\n",
    "                else:\n",
    "                    self._numXY[(x, y)] = 1\n",
    "\n",
    "        self._N = len(self._samples)\n",
    "        self._n = len(self._numXY)\n",
    "        self._C = max([len(sample) - 1 for sample in self._samples])\n",
    "        self._w = [0] * self._n\n",
    "        self._lastw = self._w[:]\n",
    "\n",
    "        self._Ep_ = [0] * self._n\n",
    "        for i, xy in enumerate(self._numXY):  # 计算特征函数fi关于经验分布的期望\n",
    "            self._Ep_[i] = self._numXY[xy] / self._N\n",
    "            self._xyID[xy] = i\n",
    "            self._IDxy[i] = xy\n",
    "\n",
    "    def _Zx(self, X):  # 计算每个Z(x)值\n",
    "        zx = 0\n",
    "        for y in self._Y:\n",
    "            ss = 0\n",
    "            for x in X:\n",
    "                if (x, y) in self._numXY:\n",
    "                    ss += self._w[self._xyID[(x, y)]]\n",
    "            zx += math.exp(ss)\n",
    "        return zx\n",
    "\n",
    "    def _model_pyx(self, y, X):  # 计算每个P(y|x)\n",
    "        zx = self._Zx(X)\n",
    "        ss = 0\n",
    "        for x in X:\n",
    "            if (x, y) in self._numXY:\n",
    "                ss += self._w[self._xyID[(x, y)]]\n",
    "        pyx = math.exp(ss) / zx\n",
    "        return pyx\n",
    "\n",
    "    def _model_ep(self, index):  # 计算特征函数fi关于模型的期望\n",
    "        x, y = self._IDxy[index]\n",
    "        ep = 0\n",
    "        for sample in self._samples:\n",
    "            if x not in sample:\n",
    "                continue\n",
    "            pyx = self._model_pyx(y, sample)\n",
    "            ep += pyx / self._N\n",
    "        return ep\n",
    "\n",
    "    def _convergence(self):  # 判断是否全部收敛\n",
    "        for last, now in zip(self._lastw, self._w):\n",
    "            if abs(last - now) >= self._EPS:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def predict(self, X):  # 计算预测概率\n",
    "        Z = self._Zx(X)\n",
    "        result = {}\n",
    "        for y in self._Y:\n",
    "            ss = 0\n",
    "            for x in X:\n",
    "                if (x, y) in self._numXY:\n",
    "                    ss += self._w[self._xyID[(x, y)]]\n",
    "            pyx = math.exp(ss) / Z\n",
    "            result[y] = pyx\n",
    "        return result\n",
    "\n",
    "    def train(self, maxiter=1000):  # 训练数据\n",
    "        for loop in range(maxiter):  # 最大训练次数\n",
    "            print(\"iter:%d\" % loop)\n",
    "            self._lastw = self._w[:]\n",
    "            for i in range(self._n):\n",
    "                ep = self._model_ep(i)  # 计算第i个特征的模型期望\n",
    "                self._w[i] += math.log(self._Ep_[i] / ep) / self._C  # 更新参数\n",
    "            print(\"w:\", self._w)\n",
    "            if self._convergence():  # 判断是否收敛\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = [['no', 'sunny', 'hot', 'high', 'FALSE'],\n",
    "           ['no', 'sunny', 'hot', 'high', 'TRUE'],\n",
    "           ['yes', 'overcast', 'hot', 'high', 'FALSE'],\n",
    "           ['yes', 'rainy', 'mild', 'high', 'FALSE'],\n",
    "           ['yes', 'rainy', 'cool', 'normal', 'FALSE'],\n",
    "           ['no', 'rainy', 'cool', 'normal', 'TRUE'],\n",
    "           ['yes', 'overcast', 'cool', 'normal', 'TRUE'],\n",
    "           ['no', 'sunny', 'mild', 'high', 'FALSE'],\n",
    "           ['yes', 'sunny', 'cool', 'normal', 'FALSE'],\n",
    "           ['yes', 'rainy', 'mild', 'normal', 'FALSE'],\n",
    "           ['yes', 'sunny', 'mild', 'normal', 'TRUE'],\n",
    "           ['yes', 'overcast', 'mild', 'high', 'TRUE'],\n",
    "           ['yes', 'overcast', 'hot', 'normal', 'FALSE'],\n",
    "           ['no', 'rainy', 'mild', 'high', 'TRUE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "maxent = MaxEntropy()\n",
    "x = ['overcast', 'mild', 'high', 'FALSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "maxent.loadData(dataset)\n",
    "maxent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('predict:', maxent.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第6章Logistic回归与最大熵模型-习题\n",
    "\n",
    "### 习题6.1\n",
    "&emsp;&emsp;确认Logistic分布属于指数分布族。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**  \n",
    "**第1步：**  \n",
    "首先给出指数分布族的定义：  \n",
    "对于随机变量$x$，在给定参数$\\eta$下，其概率分别满足如下形式：$$p(x|\\eta)=h(x)g(\\eta)\\exp(\\eta^Tu(x))$$我们称之为**指数分布族**。  \n",
    "其中：  \n",
    "$x$：可以是标量或者向量，可以是离散值也可以是连续值  \n",
    "$\\eta$：自然参数  \n",
    "$g(\\eta)$：归一化系数  \n",
    "$h(x),u(x)$：$x$的某个函数  \n",
    "\n",
    "----\n",
    "\n",
    "**第2步：**证明伯努利分布属于指数分布族  \n",
    "伯努利分布：$\\varphi$是$y=1$的概率，即$P(Y=1)=\\varphi$  \n",
    "$\\begin{aligned}\n",
    "P(y|\\varphi) \n",
    "&= \\varphi^y (1-\\varphi)^{(1-y)} \\\\\n",
    "&= (1-\\varphi) \\varphi^y (1-\\varphi)^{(-y)} \\\\\n",
    "&= (1-\\varphi) (\\frac{\\varphi}{1-\\varphi})^y \\\\\n",
    "&= (1-\\varphi) \\exp\\left(y \\ln \\frac{\\varphi}{1-\\varphi} \\right) \\\\\n",
    "&= \\frac{1}{1+e^\\eta} \\exp (\\eta y)\n",
    "\\end{aligned}$  \n",
    "其中，$\\displaystyle \\eta=\\ln \\frac{\\varphi}{1-\\varphi} \\Leftrightarrow \\varphi = \\frac{1}{1+e^{-\\eta}}$  \n",
    "将$y$替换成$x$，可得$\\displaystyle P(x|\\eta) = \\frac{1}{1+e^\\eta} \\exp (\\eta x)$\n",
    "对比可知，伯努利分布属于指数分布族，其中$\\displaystyle h(x) = 1, g(\\eta)= \\frac{1}{1+e^\\eta}, u(x)=x$  \n",
    "\n",
    "----\n",
    "\n",
    "**第3步：**  \n",
    "广义线性模型（GLM）必须满足三个假设：\n",
    "1. $y | x;\\theta \\sim ExponentialFamily(\\eta)$，即假设预测变量$y$在给定$x$，以$\\theta$为参数的条件概率下，属于以$\\eta$作为自然参数的指数分布族；  \n",
    "2. 给定$x$，求解出以$x$为条件的$T(y)$的期望$E[T(y)|x]$，即算法输出为$h(x)=E[T(y)|x]$  \n",
    "3. 满足$\\eta=\\theta^T x$，即自然参数和输入特征向量$x$之间线性相关，关系由$\\theta$决定，仅当$\\eta$是实数时才有意义，若$\\eta$是一个向量，则$\\eta_i=\\theta_i^T x$\n",
    "\n",
    "----\n",
    "\n",
    "**第4步：**推导伯努利分布的GLM  \n",
    "已知伯努利分布属于指数分布族，对给定的$x,\\eta$，求解期望：$$\\begin{aligned}\n",
    "h_{\\theta}(x) \n",
    "&= E[y|x;\\theta] \\\\\n",
    "&= 1 \\cdot p(y=1)+ 0 \\cdot p(y=0) \\\\\n",
    "&= \\varphi \\\\\n",
    "&= \\frac{1}{1+e^{-\\eta}} \\\\\n",
    "&= \\frac{1}{1+e^{-\\theta^T x}}\n",
    "\\end{aligned}$$可得到Logistic回归算法，故Logistic分布属于指数分布族，得证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题6.2\n",
    "&emsp;&emsp;写出Logistic回归模型学习的梯度下降算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**  \n",
    "对于Logistic模型：$$P(Y=1 | x)=\\frac{\\exp (w \\cdot x+b)}{1+\\exp (w \\cdot x+b)} \\\\ P(Y=0 | x)=\\frac{1}{1+\\exp (w \\cdot x+b)}\n",
    "$$对数似然函数为：$\\displaystyle L(w)=\\sum_{i=1}^N \\left[y_i (w \\cdot x_i)-\\log \\left(1+\\exp (w \\cdot x_i)\\right)\\right]$  \n",
    "似然函数求偏导，可得$\\displaystyle \\frac{\\partial L(w)}{\\partial w^{(j)}}=\\sum_{i=1}^N\\left[x_i^{(j)} \\cdot y_i-\\frac{\\exp (w \\cdot x_i) \\cdot x_i^{(j)}}{1+\\exp (w \\cdot x_i)}\\right]$  \n",
    "梯度函数为：$\\displaystyle \\nabla L(w)=\\left[\\frac{\\partial L(w)}{\\partial w^{(0)}}, \\cdots, \\frac{\\partial L(w)}{\\partial w^{(m)}}\\right]$  \n",
    "Logistic回归模型学习的梯度下降算法：  \n",
    "(1) 取初始值$x^{(0)} \\in R$，置$k=0$  \n",
    "(2) 计算$f(x^{(k)})$  \n",
    "(3) 计算梯度$g_k=g(x^{(k)})$，当$\\|g_k\\| < \\varepsilon$时，停止迭代，令$x^* = x^{(k)}$；否则，求$\\lambda_k$，使得$\\displaystyle f(x^{(k)}+\\lambda_k g_k) = \\max_{\\lambda \\geqslant 0}f(x^{(k)}+\\lambda g_k)$  \n",
    "(4) 置$x^{(k+1)}=x^{(k)}+\\lambda_k g_k$，计算$f(x^{(k+1)})$，当$\\|f(x^{(k+1)}) - f(x^{(k)})\\| < \\varepsilon$或 $\\|x^{(k+1)} - x^{(k)}\\| < \\varepsilon$时，停止迭代，令$x^* = x^{(k+1)}$  \n",
    "(5) 否则，置$k=k+1$，转(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import mpl\n",
    "\n",
    "# 图像显示中文\n",
    "mpl.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learn_rate=0.1, max_iter=10000, tol=1e-2):\n",
    "        self.learn_rate = learn_rate  # 学习率\n",
    "        self.max_iter = max_iter  # 迭代次数\n",
    "        self.tol = tol  # 迭代停止阈值\n",
    "        self.w = None  # 权重\n",
    "\n",
    "    def preprocessing(self, X):\n",
    "        \"\"\"将原始X末尾加上一列，该列数值全部为1\"\"\"\n",
    "        row = X.shape[0]\n",
    "        y = np.ones(row).reshape(row, 1)\n",
    "        X_prepro = np.hstack((X, y))\n",
    "        return X_prepro\n",
    "\n",
    "    def sigmod(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X = self.preprocessing(X_train)\n",
    "        y = y_train.T\n",
    "        # 初始化权重w\n",
    "        self.w = np.array([[0] * X.shape[1]], dtype=np.float)\n",
    "        k = 0\n",
    "        for loop in range(self.max_iter):\n",
    "            # 计算梯度\n",
    "            z = np.dot(X, self.w.T)\n",
    "            grad = X * (y - self.sigmod(z))\n",
    "            grad = grad.sum(axis=0)\n",
    "            # 利用梯度的绝对值作为迭代中止的条件\n",
    "            if (np.abs(grad) <= self.tol).all():\n",
    "                break\n",
    "            else:\n",
    "                # 更新权重w 梯度上升——求极大值\n",
    "                self.w += self.learn_rate * grad\n",
    "                k += 1\n",
    "        print(\"迭代次数：{}次\".format(k))\n",
    "        print(\"最终梯度：{}\".format(grad))\n",
    "        print(\"最终权重：{}\".format(self.w[0]))\n",
    "\n",
    "    def predict(self, x):\n",
    "        p = self.sigmod(np.dot(self.preprocessing(x), self.w.T))\n",
    "        print(\"Y=1的概率被估计为：{:.2%}\".format(p[0][0]))  # 调用score时，注释掉\n",
    "        p[np.where(p > 0.5)] = 1\n",
    "        p[np.where(p < 0.5)] = 0\n",
    "        return p\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_c = self.predict(X)\n",
    "        error_rate = np.sum(np.abs(y_c - y.T)) / y_c.shape[0]\n",
    "        return 1 - error_rate\n",
    "\n",
    "    def draw(self, X, y):\n",
    "        # 分离正负实例点\n",
    "        y = y[0]\n",
    "        X_po = X[np.where(y == 1)]\n",
    "        X_ne = X[np.where(y == 0)]\n",
    "        # 绘制数据集散点图\n",
    "        ax = plt.axes(projection='3d')\n",
    "        x_1 = X_po[0, :]\n",
    "        y_1 = X_po[1, :]\n",
    "        z_1 = X_po[2, :]\n",
    "        x_2 = X_ne[0, :]\n",
    "        y_2 = X_ne[1, :]\n",
    "        z_2 = X_ne[2, :]\n",
    "        ax.scatter(x_1, y_1, z_1, c=\"r\", label=\"正实例\")\n",
    "        ax.scatter(x_2, y_2, z_2, c=\"b\", label=\"负实例\")\n",
    "        ax.legend(loc='best')\n",
    "        # 绘制p=0.5的区分平面\n",
    "        x = np.linspace(-3, 3, 3)\n",
    "        y = np.linspace(-3, 3, 3)\n",
    "        x_3, y_3 = np.meshgrid(x, y)\n",
    "        a, b, c, d = self.w[0]\n",
    "        z_3 = -(a * x_3 + b * y_3 + d) / c\n",
    "        ax.plot_surface(x_3, y_3, z_3, alpha=0.5)  # 调节透明度\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 训练数据集\n",
    "X_train = np.array([[3, 3, 3], [4, 3, 2], [2, 1, 2], [1, 1, 1], [-1, 0, 1],\n",
    "                    [2, -2, 1]])\n",
    "y_train = np.array([[1, 1, 1, 0, 0, 0]])\n",
    "# 构建实例，进行训练\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.draw(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题6.3\n",
    "&emsp;&emsp;写出最大熵模型学习的DFP算法。（关于一般的DFP算法参见附录B）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**  \n",
    "**第1步：**  \n",
    "最大熵模型为：$$\n",
    "\\begin{array}{cl}\n",
    "{\\max } & {H(p)=-\\sum_{x, y} P(x) P(y | x) \\log P(y | x)} \\\\ \n",
    "{\\text {st.}} &\n",
    "{E_p(f_i)-E_{\\hat{p}}(f_i)=0, \\quad i=1,2, \\cdots,n} \\\\ \n",
    "& {\\sum_y P(y | x)=1}\n",
    "\\end{array}$$引入拉格朗日乘子，定义拉格朗日函数：$$\n",
    "L(P, w)=\\sum_{xy} P(x) P(y | x) \\log P(y | x)+w_0 \\left(1-\\sum_y P(y | x)\\right) \\\\\n",
    "+\\sum_{i=1} w_i\\left(\\sum_{xy} P(x, y) f_i(x, y)-\\sum_{xy} P(x, y) P(y | x) f_i(x, y)\\right)$$\n",
    "最优化原始问题为：$$\\min_{P \\in C} \\max_{w} L(P,w)$$对偶问题为：$$\\max_{w} \\min_{P \\in C} L(P,w)$$令$$\\Psi(w) = \\min_{P \\in C} L(P,w) = L(P_w, w)$$$\\Psi(w)$称为对偶函数，同时，其解记作$$P_w = \\mathop{\\arg \\min}_{P \\in C} L(P,w) = P_w(y|x)$$求$L(P,w)$对$P(y|x)$的偏导数，并令偏导数等于0，解得：$$P_w(y | x)=\\frac{1}{Z_w(x)} \\exp \\left(\\sum_{i=1}^n w_i f_i (x, y)\\right)$$其中：$$Z_w(x)=\\sum_y \\exp \\left(\\sum_{i=1}^n w_i f_i(x, y)\\right)$$则最大熵模型目标函数表示为$$\\varphi(w)=\\min_{w \\in R_n} \\Psi(w) = \\sum_{x} P(x) \\log \\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)-\\sum_{x, y} P(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第2步：**  \n",
    "DFP的$G_{k+1}$的迭代公式为：$$G_{k+1}=G_k+\\frac{\\delta_k \\delta_k^T}{\\delta_k^T y_k}-\\frac{G_k y_k y_k^T G_k}{y_k^T G_k y_k}$$  \n",
    "**最大熵模型的DFP算法：**   \n",
    "输入：目标函数$\\varphi(w)$，梯度$g(w) = \\nabla g(w)$，精度要求$\\varepsilon$；  \n",
    "输出：$\\varphi(w)$的极小值点$w^*$  \n",
    "(1)选定初始点$w^{(0)}$，取$G_0$为正定对称矩阵，置$k=0$  \n",
    "(2)计算$g_k=g(w^{(k)})$，若$\\|g_k\\| < \\varepsilon$，则停止计算，得近似解$w^*=w^{(k)}$，否则转(3)  \n",
    "(3)置$p_k=-G_kg_k$  \n",
    "(4)一维搜索：求$\\lambda_k$使得$$\\varphi\\left(w^{(k)}+\\lambda_k P_k\\right)=\\min _{\\lambda \\geqslant 0} \\varphi\\left(w^{(k)}+\\lambda P_{k}\\right)$$(5)置$w^{(k+1)}=w^{(k)}+\\lambda_k p_k$  \n",
    "(6)计算$g_{k+1}=g(w^{(k+1)})$，若$\\|g_{k+1}\\| < \\varepsilon$，则停止计算，得近似解$w^*=w^{(k+1)}$；否则，按照迭代式算出$G_{k+1}$  \n",
    "(7)置$k=k+1$，转(3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "参考代码：https://github.com/wzyonggege/statistical-learning-method\n",
    "\n",
    "本文代码更新地址：https://github.com/fengdu78/lihang-code\n",
    "\n",
    "习题解答：https://github.com/datawhalechina/statistical-learning-method-solutions-manual\n",
    "\n",
    "中文注释制作：机器学习初学者公众号：ID:ai-start-com\n",
    "\n",
    "配置环境：python 3.5+\n",
    "\n",
    "代码全部测试通过。\n",
    "![gongzhong](../gongzhong.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}